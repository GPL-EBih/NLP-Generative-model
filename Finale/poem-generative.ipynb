{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-08-07T09:07:55.666781Z","iopub.status.busy":"2024-08-07T09:07:55.666020Z","iopub.status.idle":"2024-08-07T09:07:57.411016Z","shell.execute_reply":"2024-08-07T09:07:57.409981Z","shell.execute_reply.started":"2024-08-07T09:07:55.666747Z"},"id":"WI1tduG8Sjgv","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["--2024-08-07 09:07:55--  https://huggingface.co/datasets/Libosa2707/vietnamese-poem/resolve/main/poems_dataset.csv\n","Resolving huggingface.co (huggingface.co)... 18.239.69.83, 18.239.69.50, 18.239.69.31, ...\n","Connecting to huggingface.co (huggingface.co)|18.239.69.83|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://cdn-lfs.huggingface.co/repos/f1/ab/f1ab000118e9cde67ceda9e6bc227f8bf8dfa736de9307d30ea2f95af2ebf69c/0248dcd6e6442c76f66b61088dc6f4cee92b62f386bdbae649a248f9aa17df80?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27poems_dataset.csv%3B+filename%3D%22poems_dataset.csv%22%3B&response-content-type=text%2Fcsv&Expires=1723280875&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMzI4MDg3NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9mMS9hYi9mMWFiMDAwMTE4ZTljZGU2N2NlZGE5ZTZiYzIyN2Y4YmY4ZGZhNzM2ZGU5MzA3ZDMwZWEyZjk1YWYyZWJmNjljLzAyNDhkY2Q2ZTY0NDJjNzZmNjZiNjEwODhkYzZmNGNlZTkyYjYyZjM4NmJkYmFlNjQ5YTI0OGY5YWExN2RmODA%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=MPAdh1d6Lh9xquQevrDJGia%7EruLNRj6eTjwCbLhZw05Ph8HmnoU6Cs1um9BIjar8yzoiV95-t682hkSwbMFQBHMVnNGZoW2-uIvF1urLSr8%7EAsukSmb099SZTYbqwIpkwwPRMfS6DgcXzF1EV%7ETegpIXtBP2R82MjjKK4i4f4ynUh9sMGlhRmCrD-eF1pbfKlPEV1QsaX33MyCyKwHvtc7%7E3R1CogidcXGOlKv4wODrXERHbmsAfv4JHbWvyxq54acgsuKfEGfOqPdWhEDeLRHMR5Plo-1GBt2oYbhH85fWUT2mwv5QX-7NM0XJldmjamjWDYgxsis8QSlmtMsq8NA__&Key-Pair-Id=K3ESJI6DHPFC7 [following]\n","--2024-08-07 09:07:55--  https://cdn-lfs.huggingface.co/repos/f1/ab/f1ab000118e9cde67ceda9e6bc227f8bf8dfa736de9307d30ea2f95af2ebf69c/0248dcd6e6442c76f66b61088dc6f4cee92b62f386bdbae649a248f9aa17df80?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27poems_dataset.csv%3B+filename%3D%22poems_dataset.csv%22%3B&response-content-type=text%2Fcsv&Expires=1723280875&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMzI4MDg3NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9mMS9hYi9mMWFiMDAwMTE4ZTljZGU2N2NlZGE5ZTZiYzIyN2Y4YmY4ZGZhNzM2ZGU5MzA3ZDMwZWEyZjk1YWYyZWJmNjljLzAyNDhkY2Q2ZTY0NDJjNzZmNjZiNjEwODhkYzZmNGNlZTkyYjYyZjM4NmJkYmFlNjQ5YTI0OGY5YWExN2RmODA%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=MPAdh1d6Lh9xquQevrDJGia%7EruLNRj6eTjwCbLhZw05Ph8HmnoU6Cs1um9BIjar8yzoiV95-t682hkSwbMFQBHMVnNGZoW2-uIvF1urLSr8%7EAsukSmb099SZTYbqwIpkwwPRMfS6DgcXzF1EV%7ETegpIXtBP2R82MjjKK4i4f4ynUh9sMGlhRmCrD-eF1pbfKlPEV1QsaX33MyCyKwHvtc7%7E3R1CogidcXGOlKv4wODrXERHbmsAfv4JHbWvyxq54acgsuKfEGfOqPdWhEDeLRHMR5Plo-1GBt2oYbhH85fWUT2mwv5QX-7NM0XJldmjamjWDYgxsis8QSlmtMsq8NA__&Key-Pair-Id=K3ESJI6DHPFC7\n","Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.239.18.94, 18.239.18.29, 18.239.18.84, ...\n","Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.239.18.94|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 122519099 (117M) [text/csv]\n","Saving to: ‘poems_dataset.csv.2’\n","\n","poems_dataset.csv.2 100%[===================>] 116.84M   237MB/s    in 0.5s    \n","\n","2024-08-07 09:07:56 (237 MB/s) - ‘poems_dataset.csv.2’ saved [122519099/122519099]\n","\n","--2024-08-07 09:07:56--  https://huggingface.co/datasets/phamson02/vietnamese-poetry-corpus/resolve/main/poems_dataset.csv\n","Resolving huggingface.co (huggingface.co)... 18.239.69.50, 18.239.69.71, 18.239.69.31, ...\n","Connecting to huggingface.co (huggingface.co)|18.239.69.50|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://cdn-lfs.huggingface.co/repos/dc/50/dc50670e226398405ccec94bcae7d5139528c8107e7f7d426a747e8550c3e2d8/f96c64f218ce63ce863e5360df2904e3415a2d1d1706d4e2de2d7c06c9e5c0b3?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27poems_dataset.csv%3B+filename%3D%22poems_dataset.csv%22%3B&response-content-type=text%2Fcsv&Expires=1723280876&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMzI4MDg3Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9kYy81MC9kYzUwNjcwZTIyNjM5ODQwNWNjZWM5NGJjYWU3ZDUxMzk1MjhjODEwN2U3ZjdkNDI2YTc0N2U4NTUwYzNlMmQ4L2Y5NmM2NGYyMThjZTYzY2U4NjNlNTM2MGRmMjkwNGUzNDE1YTJkMWQxNzA2ZDRlMmRlMmQ3YzA2YzllNWMwYjM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=WTHixhyX8HJGjLRDEW%7ENL3byUxupxxUXPSKZCykCktZiw4KveWe3q50c0cJjz-pbCJ63LIANspcEp2xNXWir4MiEmiCsOjg0reUrwpHtII4NOlS1dJQgKDuungdY6vkgZ4XVIfXYH087k6bvtW5emlr4AalAdwoq8WegUk%7EZFQjwUlC6dQs8fgqAJyMXcvE0F5Yu7uUjwnAujL9zc7AJKOE6TlNSmGlTw5EM6YRQRFPDbKUDzNCseIp0FeKuFLAxZOatn2ASlWwNDi0Cs84HmwZqFc8wvMeK7StmsbTplAmcjYGsp5HwS4-6jMZ9aHjIYpo8Af1Nh9oN17Knm9NQUA__&Key-Pair-Id=K3ESJI6DHPFC7 [following]\n","--2024-08-07 09:07:56--  https://cdn-lfs.huggingface.co/repos/dc/50/dc50670e226398405ccec94bcae7d5139528c8107e7f7d426a747e8550c3e2d8/f96c64f218ce63ce863e5360df2904e3415a2d1d1706d4e2de2d7c06c9e5c0b3?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27poems_dataset.csv%3B+filename%3D%22poems_dataset.csv%22%3B&response-content-type=text%2Fcsv&Expires=1723280876&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMzI4MDg3Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9kYy81MC9kYzUwNjcwZTIyNjM5ODQwNWNjZWM5NGJjYWU3ZDUxMzk1MjhjODEwN2U3ZjdkNDI2YTc0N2U4NTUwYzNlMmQ4L2Y5NmM2NGYyMThjZTYzY2U4NjNlNTM2MGRmMjkwNGUzNDE1YTJkMWQxNzA2ZDRlMmRlMmQ3YzA2YzllNWMwYjM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=WTHixhyX8HJGjLRDEW%7ENL3byUxupxxUXPSKZCykCktZiw4KveWe3q50c0cJjz-pbCJ63LIANspcEp2xNXWir4MiEmiCsOjg0reUrwpHtII4NOlS1dJQgKDuungdY6vkgZ4XVIfXYH087k6bvtW5emlr4AalAdwoq8WegUk%7EZFQjwUlC6dQs8fgqAJyMXcvE0F5Yu7uUjwnAujL9zc7AJKOE6TlNSmGlTw5EM6YRQRFPDbKUDzNCseIp0FeKuFLAxZOatn2ASlWwNDi0Cs84HmwZqFc8wvMeK7StmsbTplAmcjYGsp5HwS4-6jMZ9aHjIYpo8Af1Nh9oN17Knm9NQUA__&Key-Pair-Id=K3ESJI6DHPFC7\n","Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.239.18.29, 18.239.18.68, 18.239.18.94, ...\n","Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.239.18.29|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 163593426 (156M) [text/csv]\n","Saving to: ‘poems_dataset.csv.3’\n","\n","poems_dataset.csv.3 100%[===================>] 156.01M   262MB/s    in 0.6s    \n","\n","2024-08-07 09:07:57 (262 MB/s) - ‘poems_dataset.csv.3’ saved [163593426/163593426]\n","\n"]}],"source":["!wget https://huggingface.co/datasets/Libosa2707/vietnamese-poem/resolve/main/poems_dataset.csv\n","!wget https://huggingface.co/datasets/phamson02/vietnamese-poetry-corpus/resolve/main/poems_dataset.csv"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T11:06:36.316503Z","iopub.status.busy":"2024-08-07T11:06:36.315834Z","iopub.status.idle":"2024-08-07T11:06:51.105396Z","shell.execute_reply":"2024-08-07T11:06:51.104466Z","shell.execute_reply.started":"2024-08-07T11:06:36.316471Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting underthesea\n","  Downloading underthesea-6.8.4-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: Click>=6.0 in /opt/conda/lib/python3.10/site-packages (from underthesea) (8.1.7)\n","Collecting python-crfsuite>=0.9.6 (from underthesea)\n","  Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n","Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from underthesea) (3.2.4)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from underthesea) (4.66.4)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from underthesea) (2.32.3)\n","Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from underthesea) (1.4.2)\n","Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from underthesea) (1.2.2)\n","Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from underthesea) (6.0.1)\n","Collecting underthesea-core==1.0.4 (from underthesea)\n","  Downloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl.metadata (1.7 kB)\n","Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->underthesea) (1.16.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (2024.7.4)\n","Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->underthesea) (1.26.4)\n","Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->underthesea) (1.11.4)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->underthesea) (3.2.0)\n","Downloading underthesea-6.8.4-py3-none-any.whl (20.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl (657 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m657.8/657.8 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: underthesea-core, python-crfsuite, underthesea\n","Successfully installed python-crfsuite-0.9.10 underthesea-6.8.4 underthesea-core-1.0.4\n"]}],"source":["!pip install underthesea\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T11:06:51.107489Z","iopub.status.busy":"2024-08-07T11:06:51.107173Z","iopub.status.idle":"2024-08-07T11:06:58.455457Z","shell.execute_reply":"2024-08-07T11:06:58.454632Z","shell.execute_reply.started":"2024-08-07T11:06:51.107463Z"},"id":"JkPtBYeXSjgx","trusted":true},"outputs":[],"source":["import os\n","import re\n","import time\n","import pandas as pd\n","import torch\n","import numpy as np\n","import torch.nn as nn\n","from tqdm import tqdm\n","import torch.nn.functional as F\n","from sklearn.model_selection import train_test_split\n","from underthesea import word_tokenize\n","import pickle"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T09:08:05.800137Z","iopub.status.busy":"2024-08-07T09:08:05.799568Z","iopub.status.idle":"2024-08-07T09:08:05.804600Z","shell.execute_reply":"2024-08-07T09:08:05.803970Z","shell.execute_reply.started":"2024-08-07T09:08:05.800104Z"},"id":"CSdGd9HXSjgx","trusted":true},"outputs":[],"source":["def save_pkl(save_object, save_file):\n","    with open(save_file, 'wb') as f:\n","        pickle.dump(save_object, f, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","def load_pkl(load_file):\n","    with open(load_file, 'rb') as f:\n","        output = pickle.load(f)\n","    return output"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T09:08:05.806707Z","iopub.status.busy":"2024-08-07T09:08:05.806456Z","iopub.status.idle":"2024-08-07T09:08:08.675855Z","shell.execute_reply":"2024-08-07T09:08:08.675116Z","shell.execute_reply.started":"2024-08-07T09:08:05.806683Z"},"id":"F-nGqQdKSjgy","trusted":true},"outputs":[],"source":["data = pd.read_csv(\"poems_dataset.csv\")"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T09:08:08.677026Z","iopub.status.busy":"2024-08-07T09:08:08.676780Z","iopub.status.idle":"2024-08-07T09:08:08.692791Z","shell.execute_reply":"2024-08-07T09:08:08.692114Z","shell.execute_reply.started":"2024-08-07T09:08:08.677003Z"},"id":"xc_cCEU5Sjgy","outputId":"92324dba-f744-410b-a210-e308505e1787","trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>content</th>\n","      <th>title</th>\n","      <th>url</th>\n","      <th>genre</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>139248</td>\n","      <td>ngày đông se sắt lạnh trong lòng\\ncó việc đi n...</td>\n","      <td>SAY NẮNG</td>\n","      <td>https://www.facebook.com/groups/48640773509859...</td>\n","      <td>7 chu</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>139249</td>\n","      <td>ôm đàn thao thức đến nữa đêm\\nréo rắt cung âm ...</td>\n","      <td>NaN</td>\n","      <td>https://www.facebook.com/groups/17645444269765...</td>\n","      <td>7 chu</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>139250</td>\n","      <td>tết có người vui có kẻ buồn\\nngười cười toe to...</td>\n","      <td>TẾT HAI THÁI CỰC</td>\n","      <td>https://www.facebook.com/groups/17645444269765...</td>\n","      <td>7 chu</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>139251</td>\n","      <td>đã quá ba mươi mộng lỡ làng\\nđi tìm day dứt mả...</td>\n","      <td>TRÁI NGANG ĐỨC HẠNH</td>\n","      <td>https://www.facebook.com/groups/48640773509859...</td>\n","      <td>7 chu</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>139252</td>\n","      <td>mai đào nở rộ đón nàng xuân\\nsợi nắng hanh vàn...</td>\n","      <td>DÁNG XUÂN</td>\n","      <td>https://www.facebook.com/groups/17645444269765...</td>\n","      <td>7 chu</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       id                                            content  \\\n","0  139248  ngày đông se sắt lạnh trong lòng\\ncó việc đi n...   \n","1  139249  ôm đàn thao thức đến nữa đêm\\nréo rắt cung âm ...   \n","2  139250  tết có người vui có kẻ buồn\\nngười cười toe to...   \n","3  139251  đã quá ba mươi mộng lỡ làng\\nđi tìm day dứt mả...   \n","4  139252  mai đào nở rộ đón nàng xuân\\nsợi nắng hanh vàn...   \n","\n","                 title                                                url  \\\n","0             SAY NẮNG  https://www.facebook.com/groups/48640773509859...   \n","1                  NaN  https://www.facebook.com/groups/17645444269765...   \n","2     TẾT HAI THÁI CỰC  https://www.facebook.com/groups/17645444269765...   \n","3  TRÁI NGANG ĐỨC HẠNH  https://www.facebook.com/groups/48640773509859...   \n","4            DÁNG XUÂN  https://www.facebook.com/groups/17645444269765...   \n","\n","   genre  \n","0  7 chu  \n","1  7 chu  \n","2  7 chu  \n","3  7 chu  \n","4  7 chu  "]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["data.head()"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T09:08:08.694051Z","iopub.status.busy":"2024-08-07T09:08:08.693750Z","iopub.status.idle":"2024-08-07T09:08:08.731549Z","shell.execute_reply":"2024-08-07T09:08:08.730788Z","shell.execute_reply.started":"2024-08-07T09:08:08.694025Z"},"id":"fh2XVC0oSjgz","trusted":true},"outputs":[],"source":["train_df, val_df = train_test_split(data, test_size=0.3, random_state=42)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T09:08:08.732705Z","iopub.status.busy":"2024-08-07T09:08:08.732478Z","iopub.status.idle":"2024-08-07T09:08:08.758397Z","shell.execute_reply":"2024-08-07T09:08:08.757707Z","shell.execute_reply.started":"2024-08-07T09:08:08.732682Z"},"id":"iDcw8UOCSjgz","trusted":true},"outputs":[],"source":["train_documents = [doc for doc in train_df['content'].tolist()]\n","val_documents = [doc for doc in val_df['content'].tolist()]"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T11:08:07.221439Z","iopub.status.busy":"2024-08-07T11:08:07.220768Z","iopub.status.idle":"2024-08-07T11:08:07.242167Z","shell.execute_reply":"2024-08-07T11:08:07.241235Z","shell.execute_reply.started":"2024-08-07T11:08:07.221404Z"},"id":"GnUIwNGPSjgz","trusted":true},"outputs":[],"source":["class Vocabulary:\n","    \"\"\" The Vocabulary class is used to record words, which are used to convert\n","        text to numbers and vice versa.\n","    \"\"\"\n","    def __init__(self):\n","        self.word2id = dict()\n","\n","        self.pad_id = 0\n","        self.unk_id = 1\n","        self.sos_id = 2\n","        self.eos_id = 3\n","\n","        self.word2id['<pad>'] = self.pad_id # Pad Token\n","        self.word2id['<unk>'] = self.unk_id # Unknown Token\n","        self.word2id['<s>'] = self.sos_id # start token\n","        self.word2id['</s>'] = self.eos_id # end token\n","\n","        self.id2word = {v: k for k, v in self.word2id.items()}\n","\n","    def __getitem__(self, word):\n","        return self.word2id.get(word, self.unk_id)\n","\n","    def __contains__(self, word):\n","        return word in self.word2id\n","\n","    def __len__(self):\n","        return len(self.word2id)\n","\n","    def lookup_tokens(self, word_indexes: list):\n","        \"\"\"\n","        @param word_indexes (list(int))\n","        @return words (list(str))\n","        \"\"\"\n","        return [self.id2word[word_index] for word_index in word_indexes]\n","\n","    def add(self, word):\n","        \"\"\" Add word to vocabulary\n","        @param word (str)\n","        @return index (str): index of the word just added\n","        \"\"\"\n","        if word not in self:\n","            word_index = self.word2id[word] = len(self.word2id)\n","            self.id2word[word_index] = word\n","            return word_index\n","        else:\n","            return self[word]\n","\n","    def corpus_to_tensor(self, corpus, is_tokenized=False):\n","        \"\"\" Convert corpus to a list of indices tensor\n","        @param corpus (list(str) if is_tokenized==False else list(list(str)))\n","        @param is_tokenized (bool)\n","        @return indicies_corpus (list(tensor))\n","        \"\"\"\n","        if is_tokenized:\n","            tokenized_corpus = corpus\n","        else:\n","            tokenized_corpus = self.tokenize_corpus(corpus)\n","        indicies_corpus = list()\n","        for document in tqdm(tokenized_corpus):\n","            indicies_document = torch.tensor(list(map(lambda word: self[word], document)), dtype=torch.long)\n","            indicies_corpus.append(indicies_document)\n","\n","        return indicies_corpus\n","\n","    def tensor_to_corpus(self, tensor):\n","        \"\"\" Convert list of indices tensor to a list of tokenized documents\n","        @param indicies_corpus (list(tensor))\n","        @return corpus (list(list(str)))\n","        \"\"\"\n","        corpus = list()\n","        for indicies in tqdm(tensor):\n","            document = list(map(lambda index: self.id2word[index.item()], indicies))\n","            corpus.append(document)\n","\n","        return corpus\n","\n","    @staticmethod\n","    def tokenize_corpus(corpus):\n","        \"\"\"Split the documents of the corpus into words\n","        @param corpus (list(str)): list of documents\n","        @return tokenized_corpus (list(list(str))): list of words\n","        \"\"\"\n","        print(\"Tokenize the corpus...\")\n","        tokenized_corpus = list()\n","        for document in tqdm(corpus):\n","            tokenized_document = ['<s>'] + re.findall(r'(\\w+|[^\\w\\s]|\\S+|\\n)', document) + ['</s>']\n","            tokenized_corpus.append(tokenized_document)\n","\n","        return tokenized_corpus\n","\n","    @classmethod\n","    def from_documents(cls, documents):\n","        words = set(word for doc in documents for word in re.findall(r'\\w+|\\S|\\n', doc))\n","        vocab = cls()\n","        for w in words:\n","            vocab.add(w)\n","        return vocab\n","\n","    @classmethod\n","    def from_pretrained(cls, save_dir):\n","        import pickle\n","        \"\"\"\n","        Initialize a new Vocabulary from a pre-trained vocabulary saved as a pickle file.\n","\n","        Parameters:\n","        - vocab_file (str): Path to the pickle file containing the pre-trained vocabulary.\n","\n","        Returns:\n","        - vocab (Vocabulary): Initialized Vocabulary object.\n","        \"\"\"\n","        with open(os.path.join(save_dir, \"vocab.pkl\"), 'rb') as file:\n","            pretrained_vocab = pickle.load(file)\n","\n","        return cls.init_vocab_from_pretrained(pretrained_vocab)\n","\n","    @staticmethod\n","    def init_vocab_from_pretrained(pretrained_vocab):\n","        \"\"\"\n","        Initialize a new Vocabulary from a pre-trained vocabulary.\n","\n","        Parameters:\n","        - pretrained_vocab (dict): A dictionary mapping words to their corresponding indices.\n","\n","        Returns:\n","        - vocab (Vocabulary): Initialized Vocabulary object.\n","        \"\"\"\n","        vocab = Vocabulary()\n","\n","        # Copy the pre-trained word-to-index mapping to the new vocabulary\n","        vocab.word2id.update(pretrained_vocab)\n","\n","        # Create the index-to-word mapping\n","        vocab.id2word = {v: k for k, v in vocab.word2id.items()}\n","\n","        return vocab\n","\n","    def save_pretrained(self, save_dir):\n","        import os\n","        import pickle\n","        \"\"\"\n","        Save the vocabulary to a pickle file.\n","\n","        Parameters:\n","        - pickle_file_path (str): Path to the pickle file where the vocabulary will be saved.\n","        \"\"\"\n","        os.makedirs(save_dir, exist_ok=True)\n","        with open(os.path.join(save_dir, \"vocab.pkl\"), 'wb') as file:\n","            pickle.dump(self.word2id, file)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T09:08:08.780749Z","iopub.status.busy":"2024-08-07T09:08:08.780481Z","iopub.status.idle":"2024-08-07T09:08:15.433748Z","shell.execute_reply":"2024-08-07T09:08:15.432913Z","shell.execute_reply.started":"2024-08-07T09:08:08.780723Z"},"id":"45YJ80SSSjg0","trusted":true},"outputs":[],"source":["vocab = Vocabulary.from_documents(train_documents)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T11:08:10.209649Z","iopub.status.busy":"2024-08-07T11:08:10.208915Z","iopub.status.idle":"2024-08-07T11:08:10.221262Z","shell.execute_reply":"2024-08-07T11:08:10.220215Z","shell.execute_reply.started":"2024-08-07T11:08:10.209618Z"},"id":"wsqTzI0ISjg0","trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","\n","class PoemGenerationDataset(Dataset):\n","    def __init__(self, documents, vocab, max_length=None):\n","        self.vocab = vocab\n","        self.sos_idx = vocab[\"<s>\"]\n","        self.eos_idx = vocab[\"</s>\"]\n","        self.pad_idx = vocab[\"<pad>\"]\n","        self.documents = documents\n","        self.max_length = max_length\n","        self.tokenized_documents = self.vocab.tokenize_corpus(self.documents)\n","        self.tensor_data = self.vocab.corpus_to_tensor(self.tokenized_documents, is_tokenized=True)\n","\n","    def __len__(self):\n","        return len(self.tensor_data)\n","\n","    def __getitem__(self, idx):\n","        return self.tensor_data[idx]\n","\n","    def shift_right(self, input_ids, pad_token=0):\n","        \"\"\"\n","        Shifts the input tensor of batch of input ids to the right by one position.\n","\n","        Args:\n","        - input_ids (torch.Tensor): Batch of input ids with shape (batch_size, sequence_length).\n","        - pad_token (int): Token used for padding. Defaults to 0.\n","\n","        Returns:\n","        - torch.Tensor: Shifted input tensor with the same shape as input_ids.\n","        \"\"\"\n","        # Create a tensor of the same shape filled with pad_token\n","        padding_column = torch.full_like(input_ids[:, :1], pad_token)\n","\n","        # Concatenate the padding column to the left of the input_ids\n","        shifted_ids = torch.cat([padding_column, input_ids[:, :-1]], dim=-1)\n","\n","        return shifted_ids\n","\n","\n","    def collate_fn(self, examples):\n","        examples = sorted(examples, key=lambda e: len(e), reverse=True)\n","\n","        if self.max_length is not None:\n","            examples = [torch.cat([e[:self.max_length-1], torch.tensor([self.eos_idx])]) for e in examples]\n","\n","        docs = [e for e in examples]\n","        # padding tokens\n","        input_ids = torch.nn.utils.rnn.pad_sequence(docs, batch_first=True, padding_value=self.pad_idx)\n","        labels = input_ids.clone()\n","        input_ids = self.shift_right(input_ids, pad_token=self.pad_idx)\n","        return {\"inputs\": input_ids, \"labels\": labels}"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T09:08:15.445216Z","iopub.status.busy":"2024-08-07T09:08:15.444941Z","iopub.status.idle":"2024-08-07T09:08:37.497058Z","shell.execute_reply":"2024-08-07T09:08:37.496291Z","shell.execute_reply.started":"2024-08-07T09:08:15.445190Z"},"id":"hsd29TpGSjg1","outputId":"0cf6295b-3624-46b2-83e7-8dc90fef1f80","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokenize the corpus...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 119827/119827 [00:07<00:00, 15475.74it/s]\n","100%|██████████| 119827/119827 [00:08<00:00, 14959.43it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Tokenize the corpus...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 51355/51355 [00:03<00:00, 16403.65it/s]\n","100%|██████████| 51355/51355 [00:03<00:00, 16340.31it/s]\n"]}],"source":["train_dataset = PoemGenerationDataset(train_documents, vocab, max_length=512)\n","val_dataset = PoemGenerationDataset(val_documents, vocab, max_length=512)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T09:08:37.498332Z","iopub.status.busy":"2024-08-07T09:08:37.498051Z","iopub.status.idle":"2024-08-07T09:08:37.502683Z","shell.execute_reply":"2024-08-07T09:08:37.501985Z","shell.execute_reply.started":"2024-08-07T09:08:37.498306Z"},"id":"DM47OgdBSjg1","trusted":true},"outputs":[],"source":["train_dataloader = DataLoader(train_dataset, batch_size=5, collate_fn=train_dataset.collate_fn)\n","val_dataloader = DataLoader(val_dataset, batch_size=5, collate_fn=val_dataset.collate_fn)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T11:10:42.861947Z","iopub.status.busy":"2024-08-07T11:10:42.861260Z","iopub.status.idle":"2024-08-07T11:10:42.878161Z","shell.execute_reply":"2024-08-07T11:10:42.877257Z","shell.execute_reply.started":"2024-08-07T11:10:42.861915Z"},"id":"ZcbOH38ESjg1","trusted":true},"outputs":[],"source":["class LSTMForPoemGeneration(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","        self.embedding = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.padding_idx)\n","        self.batch_norm = nn.BatchNorm1d(config.hidden_size)\n","        self.lstm = nn.LSTM(config.hidden_size,\n","                            config.hidden_size,\n","                            num_layers=config.num_layers,\n","                            dropout=config.dropout,\n","                            batch_first=True)\n","        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size)\n","        self.dropout = nn.Dropout(config.dropout)\n","\n","    def forward(self, input_ids, labels=None):\n","        embeds = self.dropout(self.embedding(input_ids))\n","        bn_output = self.batch_norm(embeds.permute(0,2,1)).permute(0,2,1)\n","        output, (hidden, cell) = self.lstm(bn_output)\n","\n","        logits = self.lm_head(self.dropout(output))\n","        loss = None\n","\n","        if labels is not None:\n","            loss_fct = nn.CrossEntropyLoss()\n","            labels = labels.to(logits.device)\n","            loss = loss_fct(logits.view(-1, logits.size(2)), labels.view(-1))\n","        return logits, loss\n","\n","    @torch.no_grad()\n","    def generate(self, input_ids, max_length=100, temperature=1.0):\n","        self.to(input_ids.device)\n","        self.eval()\n","\n","        current_length = input_ids.size(1)\n","        while current_length < max_length:\n","            logits, _ = self.forward(input_ids)\n","            logits = logits[:, -1, :] / temperature\n","            probs = F.softmax(logits, dim=-1)\n","            next_token_id = torch.multinomial(probs, num_samples=1)\n","            input_ids = torch.cat([input_ids, next_token_id], dim=1)\n","            current_length += 1\n","\n","            # Check if </s> token is generated\n","            if next_token_id.item() == vocab.eos_id:\n","                break\n","\n","        return input_ids\n","\n","    def save_pretrained(self, save_dir):\n","        os.makedirs(save_dir, exist_ok=True)\n","        torch.save(self.config, os.path.join(save_dir, \"config.pt\"))\n","        torch.save(self.state_dict(), os.path.join(save_dir, \"pytorch_model.pt\"))\n","\n","    @classmethod\n","    def from_pretrained(cls, saved_dir):\n","        config = torch.load(os.path.join(saved_dir, \"config.pt\"))\n","        model = cls(config)\n","        model.load_state_dict(torch.load(os.path.join(saved_dir, \"pytorch_model.pt\")))\n","        return model"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T11:10:40.331912Z","iopub.status.busy":"2024-08-07T11:10:40.331536Z","iopub.status.idle":"2024-08-07T11:10:40.337500Z","shell.execute_reply":"2024-08-07T11:10:40.336485Z","shell.execute_reply.started":"2024-08-07T11:10:40.331881Z"},"id":"fhNd3W9jSjg2","trusted":true},"outputs":[],"source":["from dataclasses import dataclass\n","\n","@dataclass\n","class LSTMConfig:\n","    vocab_size = len(vocab)\n","    hidden_size = 512\n","    padding_idx = 0\n","    dropout = 0.05\n","    num_layers = 2\n","\n","config = LSTMConfig()"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T09:08:37.537785Z","iopub.status.busy":"2024-08-07T09:08:37.537526Z","iopub.status.idle":"2024-08-07T09:08:37.547127Z","shell.execute_reply":"2024-08-07T09:08:37.546395Z","shell.execute_reply.started":"2024-08-07T09:08:37.537753Z"},"id":"nbJtl0ZkSjg2","trusted":true},"outputs":[],"source":["# def prepare_model(config, vocab, *, w2v=None):\n","#     # Generation Model\n","#     model = LSTMForPoemGeneration(config)\n","\n","#     if w2v is not None:\n","#         for w, i in w2v.stoi.items():\n","#             model.embedding.weight.data[vocab[w]].copy_(w2v.vectors[i])\n","\n","#         model.embedding.weight.data[vocab.unk_id] = torch.zeros(config.hidden_size)\n","#         model.embedding.weight.data[vocab.pad_id] = torch.zeros(config.hidden_size)\n","\n","#     return model"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T09:08:37.548371Z","iopub.status.busy":"2024-08-07T09:08:37.548112Z","iopub.status.idle":"2024-08-07T09:08:37.729537Z","shell.execute_reply":"2024-08-07T09:08:37.728771Z","shell.execute_reply.started":"2024-08-07T09:08:37.548346Z"},"id":"lnml0RZ5Sjg2","trusted":true},"outputs":[],"source":["# model = prepare_model(config, vocab)\n","model = LSTMForPoemGeneration(config)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T09:08:37.731163Z","iopub.status.busy":"2024-08-07T09:08:37.730770Z","iopub.status.idle":"2024-08-07T09:08:37.740890Z","shell.execute_reply":"2024-08-07T09:08:37.740270Z","shell.execute_reply.started":"2024-08-07T09:08:37.731135Z"},"id":"odEmHNWWSjg2","trusted":true},"outputs":[],"source":["def train(model, train_dataloader, val_dataloader, args):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n","\n","    best_val_loss = float('inf')\n","    consecutive_no_improvement = 0\n","\n","    for epoch in range(args.num_epochs):\n","        total_loss = 0.0\n","        model.train()\n","\n","        for batch in (pbar:=tqdm(train_dataloader, desc=f\"(train) Epoch {epoch}\")):\n","            inputs, labels = batch['inputs'].to(device), batch['labels'].to(device)\n","\n","            optimizer.zero_grad()\n","\n","            logits, loss = model(inputs, labels)\n","\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n","            optimizer.step()\n","\n","            pbar.set_postfix(loss=loss.item())\n","            total_loss += loss.item()\n","\n","        average_loss = total_loss / len(train_dataloader)\n","        print(f\"Epoch {epoch}/{args.num_epochs}, Train Loss: {average_loss:.4f}\")\n","\n","        # Validation\n","        model.eval()\n","        val_loss = 0.0\n","\n","        with torch.no_grad():\n","            for val_batch in (pbar:=tqdm(val_dataloader, desc=f\"(val) Epoch {epoch}\")):\n","                val_inputs, val_labels = val_batch['inputs'].to(device), val_batch['labels'].to(device)\n","                val_logits, val_loss_batch = model(val_inputs, val_labels)\n","                pbar.set_postfix(loss=val_loss_batch.item())\n","                val_loss += val_loss_batch.item()\n","\n","        average_val_loss = val_loss / len(val_dataloader)\n","        print(f\"Epoch {epoch}/{args.num_epochs}, Validation Loss: {average_val_loss:.4f}\")\n","\n","        # Early stopping check\n","        if average_val_loss < best_val_loss:\n","            best_val_loss = average_val_loss\n","            consecutive_no_improvement = 0\n","            print(f\"Save epoch {epoch} checkpoint\")\n","            # torch.save({\"state_dict\": model.state_dict(), \"vocab\": vocab}, f\"checkpoint_{epoch}.pt\")\n","            model.save_pretrained(f\"checkpoint_{epoch}\")\n","            vocab.save_pretrained(f\"checkpoint_{epoch}\")\n","        else:\n","            consecutive_no_improvement += 1\n","\n","        if consecutive_no_improvement >= args.patience:\n","            print(f\"Early stopping after {epoch} epochs without improvement on validation set.\")\n","            break\n","\n","    print(\"Training complete.\")"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T09:08:37.742121Z","iopub.status.busy":"2024-08-07T09:08:37.741781Z","iopub.status.idle":"2024-08-07T09:08:37.756269Z","shell.execute_reply":"2024-08-07T09:08:37.755615Z","shell.execute_reply.started":"2024-08-07T09:08:37.742090Z"},"id":"0eYx10iaSjg2","trusted":true},"outputs":[],"source":["class TrainingArguments:\n","    def __init__(self):\n","        self.num_epochs = 5\n","        self.patience = 5\n","        self.lr = 1e-3\n","        self.clip = 5.0\n","\n","training_args = TrainingArguments()"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T11:00:12.986265Z","iopub.status.busy":"2024-08-07T11:00:12.985262Z","iopub.status.idle":"2024-08-07T11:01:42.034457Z","shell.execute_reply":"2024-08-07T11:01:42.033008Z","shell.execute_reply.started":"2024-08-07T11:00:12.986228Z"},"id":"mFUMKDxCSjg3","outputId":"3ed42b71-9a73-4a87-a199-67cabe720f5d","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["(train) Epoch 0:   1%|          | 253/23966 [01:28<2:18:58,  2.84it/s, loss=3.16]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[20], line 21\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, val_dataloader, args)\u001b[0m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     19\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m model(inputs, labels)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), args\u001b[38;5;241m.\u001b[39mclip)\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["train(model, train_dataloader, val_dataloader, args=training_args)"]},{"cell_type":"markdown","metadata":{"id":"C3VCD21lSjg3"},"source":["# Inference"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Vocabulary:\n","    \"\"\" The Vocabulary class is used to record words, which are used to convert\n","        text to numbers and vice versa.\n","    \"\"\"\n","    def __init__(self):\n","        self.word2id = dict()\n","\n","        self.pad_id = 0\n","        self.unk_id = 1\n","        self.sos_id = 2\n","        self.eos_id = 3\n","\n","        self.word2id['<pad>'] = self.pad_id # Pad Token\n","        self.word2id['<unk>'] = self.unk_id # Unknown Token\n","        self.word2id['<s>'] = self.sos_id # start token\n","        self.word2id['</s>'] = self.eos_id # end token\n","\n","        self.id2word = {v: k for k, v in self.word2id.items()}\n","\n","    def __getitem__(self, word):\n","        return self.word2id.get(word, self.unk_id)\n","\n","    def __contains__(self, word):\n","        return word in self.word2id\n","\n","    def __len__(self):\n","        return len(self.word2id)\n","\n","    def lookup_tokens(self, word_indexes: list):\n","        \"\"\"\n","        @param word_indexes (list(int))\n","        @return words (list(str))\n","        \"\"\"\n","        return [self.id2word[word_index] for word_index in word_indexes]\n","\n","    def add(self, word):\n","        \"\"\" Add word to vocabulary\n","        @param word (str)\n","        @return index (str): index of the word just added\n","        \"\"\"\n","        if word not in self:\n","            word_index = self.word2id[word] = len(self.word2id)\n","            self.id2word[word_index] = word\n","            return word_index\n","        else:\n","            return self[word]\n","\n","    def corpus_to_tensor(self, corpus, is_tokenized=False):\n","        \"\"\" Convert corpus to a list of indices tensor\n","        @param corpus (list(str) if is_tokenized==False else list(list(str)))\n","        @param is_tokenized (bool)\n","        @return indicies_corpus (list(tensor))\n","        \"\"\"\n","        if is_tokenized:\n","            tokenized_corpus = corpus\n","        else:\n","            tokenized_corpus = self.tokenize_corpus(corpus)\n","        indicies_corpus = list()\n","        for document in tqdm(tokenized_corpus):\n","            indicies_document = torch.tensor(list(map(lambda word: self[word], document)), dtype=torch.long)\n","            indicies_corpus.append(indicies_document)\n","\n","        return indicies_corpus\n","\n","    def tensor_to_corpus(self, tensor):\n","        \"\"\" Convert list of indices tensor to a list of tokenized documents\n","        @param indicies_corpus (list(tensor))\n","        @return corpus (list(list(str)))\n","        \"\"\"\n","        corpus = list()\n","        for indicies in tqdm(tensor):\n","            document = list(map(lambda index: self.id2word[index.item()], indicies))\n","            corpus.append(document)\n","\n","        return corpus\n","\n","    @staticmethod\n","    def tokenize_corpus(corpus):\n","        \"\"\"Split the documents of the corpus into words\n","        @param corpus (list(str)): list of documents\n","        @return tokenized_corpus (list(list(str))): list of words\n","        \"\"\"\n","        print(\"Tokenize the corpus...\")\n","        tokenized_corpus = list()\n","        for document in tqdm(corpus):\n","            tokenized_document = ['<s>'] + re.findall(r'(\\w+|[^\\w\\s]|\\S+|\\n)', document) + ['</s>']\n","            tokenized_corpus.append(tokenized_document)\n","\n","        return tokenized_corpus\n","\n","    @classmethod\n","    def from_documents(cls, documents):\n","        words = set(word for doc in documents for word in re.findall(r'\\w+|\\S|\\n', doc))\n","        vocab = cls()\n","        for w in words:\n","            vocab.add(w)\n","        return vocab\n","\n","    @classmethod\n","    def from_pretrained(cls, save_dir):\n","        import pickle\n","        \"\"\"\n","        Initialize a new Vocabulary from a pre-trained vocabulary saved as a pickle file.\n","\n","        Parameters:\n","        - vocab_file (str): Path to the pickle file containing the pre-trained vocabulary.\n","\n","        Returns:\n","        - vocab (Vocabulary): Initialized Vocabulary object.\n","        \"\"\"\n","        with open(os.path.join(save_dir, \"vocab.pkl\"), 'rb') as file:\n","            pretrained_vocab = pickle.load(file)\n","\n","        return cls.init_vocab_from_pretrained(pretrained_vocab)\n","\n","    @staticmethod\n","    def init_vocab_from_pretrained(pretrained_vocab):\n","        \"\"\"\n","        Initialize a new Vocabulary from a pre-trained vocabulary.\n","\n","        Parameters:\n","        - pretrained_vocab (dict): A dictionary mapping words to their corresponding indices.\n","\n","        Returns:\n","        - vocab (Vocabulary): Initialized Vocabulary object.\n","        \"\"\"\n","        vocab = Vocabulary()\n","\n","        # Copy the pre-trained word-to-index mapping to the new vocabulary\n","        vocab.word2id.update(pretrained_vocab)\n","\n","        # Create the index-to-word mapping\n","        vocab.id2word = {v: k for k, v in vocab.word2id.items()}\n","\n","        return vocab\n","\n","    def save_pretrained(self, save_dir):\n","        import os\n","        import pickle\n","        \"\"\"\n","        Save the vocabulary to a pickle file.\n","\n","        Parameters:\n","        - pickle_file_path (str): Path to the pickle file where the vocabulary will be saved.\n","        \"\"\"\n","        os.makedirs(save_dir, exist_ok=True)\n","        with open(os.path.join(save_dir, \"vocab.pkl\"), 'wb') as file:\n","            pickle.dump(self.word2id, file)\n","\n","\n","def tokenize(vocab, input_text):\n","    tokens = ['<s>'] + re.findall(r'\\w+|\\S|\\n', input_text)\n","    input_ids = torch.tensor([vocab[word] for word in tokens], dtype=torch.long).unsqueeze(0)\n","    return input_ids\n","\n","def generate_text(model, vocab, input_text, max_length=100, temperature=1.0):\n","    input_ids = tokenize(vocab, input_text).to(\"cuda\")\n","    output_ids = model.generate(input_ids, max_length=max_length, temperature=temperature)\n","    generated_tokens = [vocab.id2word[index.item()] for index in output_ids[0]]\n","    generated_text = ' '.join(generated_tokens[1:-1])  # Exclude <s> and </s>\n","    return generated_text\n","\n","def generate_text_with_animation(model, vocab, input_text, max_length=100, temperature=1.0):\n","    import time\n","    input_ids = tokenize(vocab, input_text).to(\"cuda\")\n","    model.to(input_ids.device)\n","    model.eval()\n","\n","    generated_text = f\"{input_text}\"\n","    current_length = input_ids.size(1)\n","\n","    print(generated_text, end=' ', flush=True)\n","\n","    with torch.no_grad():\n","        while current_length < max_length:\n","            logits, _ = model(input_ids)\n","            logits = logits[:, -1, :] / temperature\n","            probs = F.softmax(logits, dim=-1)\n","            next_token_id = torch.multinomial(probs, num_samples=1)\n","            input_ids = torch.cat([input_ids, next_token_id], dim=1)\n","            current_length += 1\n","\n","            # Check if </s> token is generated\n","            if next_token_id.item() == vocab.eos_id:\n","                break\n","\n","            generated_token = vocab.id2word[next_token_id.item()]\n","            generated_text += f\"{generated_token} \"\n","            print(generated_token, end=' ', flush=True)\n","\n","            # Simulate some processing time\n","            time.sleep(0.1)\n","\n","\n","class LSTMForPoemGeneration(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","        self.embedding = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.padding_idx)\n","        self.batch_norm = nn.BatchNorm1d(config.hidden_size)\n","        self.lstm = nn.LSTM(config.hidden_size,\n","                            config.hidden_size,\n","                            num_layers=config.num_layers,\n","                            dropout=config.dropout,\n","                            batch_first=True)\n","        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size)\n","        self.dropout = nn.Dropout(config.dropout)\n","\n","    def forward(self, input_ids, labels=None):\n","        embeds = self.dropout(self.embedding(input_ids))\n","        bn_output = self.batch_norm(embeds.permute(0,2,1)).permute(0,2,1)\n","        output, (hidden, cell) = self.lstm(bn_output)\n","\n","        logits = self.lm_head(self.dropout(output))\n","        loss = None\n","\n","        if labels is not None:\n","            loss_fct = nn.CrossEntropyLoss()\n","            labels = labels.to(logits.device)\n","            loss = loss_fct(logits.view(-1, logits.size(2)), labels.view(-1))\n","        return logits, loss\n","\n","    @torch.no_grad()\n","    def generate(self, input_ids, max_length=100, temperature=1.0):\n","        self.to(input_ids.device)\n","        self.eval()\n","\n","        current_length = input_ids.size(1)\n","        while current_length < max_length:\n","            logits, _ = self.forward(input_ids)\n","            logits = logits[:, -1, :] / temperature\n","            probs = F.softmax(logits, dim=-1)\n","            next_token_id = torch.multinomial(probs, num_samples=1)\n","            input_ids = torch.cat([input_ids, next_token_id], dim=1)\n","            current_length += 1\n","\n","            # Check if </s> token is generated\n","            if next_token_id.item() == vocab.eos_id:\n","                break\n","\n","        return input_ids\n","\n","    def save_pretrained(self, save_dir):\n","        os.makedirs(save_dir, exist_ok=True)\n","        torch.save(self.config, os.path.join(save_dir, \"config.pt\"))\n","        torch.save(self.state_dict(), os.path.join(save_dir, \"pytorch_model.pt\"))\n","\n","    @classmethod\n","    def from_pretrained(cls, saved_dir):\n","        config = torch.load(os.path.join(saved_dir, \"config.pt\"))\n","        model = cls(config)\n","        model.load_state_dict(torch.load(os.path.join(saved_dir, \"pytorch_model.pt\")))\n","        return model"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T11:07:53.170324Z","iopub.status.busy":"2024-08-07T11:07:53.169275Z","iopub.status.idle":"2024-08-07T11:07:53.182510Z","shell.execute_reply":"2024-08-07T11:07:53.181443Z","shell.execute_reply.started":"2024-08-07T11:07:53.170288Z"},"id":"hKt2k9AiSjg4","trusted":true},"outputs":[],"source":["def tokenize(vocab, input_text):\n","    tokens = ['<s>'] + re.findall(r'\\w+|\\S|\\n', input_text)\n","    input_ids = torch.tensor([vocab[word] for word in tokens], dtype=torch.long).unsqueeze(0)\n","    return input_ids\n","\n","def generate_text(model, vocab, input_text, max_length=100, temperature=1.0):\n","    input_ids = tokenize(vocab, input_text).to(\"cuda\")\n","    output_ids = model.generate(input_ids, max_length=max_length, temperature=temperature)\n","    generated_tokens = [vocab.id2word[index.item()] for index in output_ids[0]]\n","    generated_text = ' '.join(generated_tokens[1:-1])  # Exclude <s> and </s>\n","    return generated_text\n","\n","def generate_text_with_animation(model, vocab, input_text, max_length=100, temperature=1.0):\n","    import time\n","    input_ids = tokenize(vocab, input_text).to(\"cuda\")\n","    model.to(input_ids.device)\n","    model.eval()\n","\n","    generated_text = f\"{input_text}\"\n","    current_length = input_ids.size(1)\n","\n","    print(generated_text, end=' ', flush=True)\n","\n","    with torch.no_grad():\n","        while current_length < max_length:\n","            logits, _ = model(input_ids)\n","            logits = logits[:, -1, :] / temperature\n","            probs = F.softmax(logits, dim=-1)\n","            next_token_id = torch.multinomial(probs, num_samples=1)\n","            input_ids = torch.cat([input_ids, next_token_id], dim=1)\n","            current_length += 1\n","\n","            # Check if </s> token is generated\n","            if next_token_id.item() == vocab.eos_id:\n","                break\n","\n","            generated_token = vocab.id2word[next_token_id.item()]\n","            generated_text += f\"{generated_token} \"\n","            print(generated_token, end=' ', flush=True)\n","\n","            # Simulate some processing time\n","            time.sleep(0.1)\n","\n","\n"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T11:05:11.281731Z","iopub.status.busy":"2024-08-07T11:05:11.281378Z","iopub.status.idle":"2024-08-07T11:05:11.292299Z","shell.execute_reply":"2024-08-07T11:05:11.291595Z","shell.execute_reply.started":"2024-08-07T11:05:11.281704Z"},"trusted":true},"outputs":[],"source":["# import torch\n","# import torch.nn.functional as F\n","# import re\n","\n","# def tokenize(vocab, input_text):\n","#     tokens = ['<s>'] + re.findall(r'\\w+|\\S|\\n', input_text)\n","#     input_ids = torch.tensor([vocab.get(word, vocab['<unk>']) for word in tokens], dtype=torch.long).unsqueeze(0)\n","#     return input_ids\n","\n","# def generate_text(model, vocab, input_text, max_length=100, temperature=1.0):\n","#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#     input_ids = tokenize(vocab, input_text).to(device)\n","#     output_ids = model.generate(input_ids, max_length=max_length, temperature=temperature)\n","#     generated_tokens = [vocab.id2word.get(index.item(), '<unk>') for index in output_ids[0]]\n","#     generated_text = ' '.join(generated_tokens[1:-1])  # Exclude <s> and </s>\n","#     return generated_text\n","\n","# def generate_text_with_animation(model, vocab, input_text, max_length=100, temperature=1.0):\n","#     import time\n","#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#     input_ids = tokenize(vocab, input_text).to(device)\n","#     model.to(device)\n","#     model.eval()\n","\n","#     generated_text = f\"{input_text}\"\n","#     current_length = input_ids.size(1)\n","\n","#     print(generated_text, end=' ', flush=True)\n","\n","#     with torch.no_grad():\n","#         while current_length < max_length:\n","#             logits, _ = model(input_ids)\n","#             logits = logits[:, -1, :] / temperature\n","#             probs = F.softmax(logits, dim=-1)\n","#             next_token_id = torch.multinomial(probs, num_samples=1)\n","#             input_ids = torch.cat([input_ids, next_token_id], dim=1)\n","#             current_length += 1\n","\n","#             # Check if </s> token is generated\n","#             if next_token_id.item() == vocab.eos_id:\n","#                 break\n","\n","#             generated_token = vocab.id2word.get(next_token_id.item(), '<unk>')\n","#             generated_text += f\"{generated_token} \"\n","#             print(generated_token, end=' ', flush=True)\n","\n","#             # Simulate some processing time\n","#             time.sleep(0.1)\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T11:10:49.248983Z","iopub.status.busy":"2024-08-07T11:10:49.247858Z","iopub.status.idle":"2024-08-07T11:10:50.155465Z","shell.execute_reply":"2024-08-07T11:10:50.154466Z","shell.execute_reply.started":"2024-08-07T11:10:49.248941Z"},"id":"YaK9mhXtSjg4","trusted":true},"outputs":[],"source":["# 1. load pretrained vocab\n","vocab = Vocabulary.from_pretrained(\"/kaggle/input/lstm-gen-model-result/checkpoint_0\")\n","model = LSTMForPoemGeneration.from_pretrained(\"/kaggle/input/lstm-gen-model-result/checkpoint_0\")"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T11:10:51.997000Z","iopub.status.busy":"2024-08-07T11:10:51.996356Z","iopub.status.idle":"2024-08-07T11:10:52.001661Z","shell.execute_reply":"2024-08-07T11:10:52.000721Z","shell.execute_reply.started":"2024-08-07T11:10:51.996967Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n"]}],"source":["import torch\n","print(torch.cuda.is_available())\n"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T11:10:52.750296Z","iopub.status.busy":"2024-08-07T11:10:52.749649Z","iopub.status.idle":"2024-08-07T11:11:22.993939Z","shell.execute_reply":"2024-08-07T11:11:22.993140Z","shell.execute_reply.started":"2024-08-07T11:10:52.750264Z"},"id":"_fkvct51Sjg4","outputId":"e3eb78f0-b845-43ec-aa0d-3f3d57c6b698","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["mừng xuân sang lộc trùm lên \n"," tươi cười mới rạng khoẻ vui quây quần \n"," mùa xuân cùng với bão bùng \n"," mừng xuân vật chất thăng trầm khó khăn \n"," ngày xuân ba sáu thanh rằm \n"," già già tất cả xuân đang chan hòa \n"," xuân về khoe sắc xuân ba \n"," hương xuân sắc thắm xuân ngời hương xuân \n"," xuân xuân vui với xuân quân \n"," giầu chẳng như thế bình minh chói màu \n"," loài hoa đua nở nhuỵ vàng \n"," xuân về khi dịch ngập tràn hoa tươi \n"," xuân về đến tận chân trời \n"," xuân về nghỉ tới một thời gan hoa \n"," bướm vàng vui hót đông hoà \n"," xuân sang nhường phật máy nhà tự do \n"," \n"," nắng xuân dạo dưới chuông chùa \n"," xuân về thêm mối vàng hoa khoe vàng \n"," hoa tươi duyên dáng phượng vàng \n"," ông trời mưa gió kéo sang rộn ràng \n"," xuân về khoe sắc vàng vang \n"," thóc đến trú quả lộc vàng lại đông \n"," xuân về ong bướm ong ong \n"," xuân về đâm lộc ngõ vào ong vui \n"," xuân về rồi sẽ chia tay \n"," trái tim giao nhịp đón chào thầm yêu \n"," xuân yêu đến nở rất nhiều \n"," xuân về đoàn tụ sớm chiều đu đưa \n"," xuân qua xuân đến bao mươi \n"," xuân về xuân đến thêu thùa bốn phương \n"," xuân về lưu giữ trời thương \n"," lộc nhường mọi việc muôn điều đẹp tươi \n"," chúc cho tất cả mọi người \n"," chúc mừng mạnh khỏe rạng ngời đường quê "]}],"source":["generate_text_with_animation(model, vocab, \"mừng xuân sang\", max_length=512, temperature=.9)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5508289,"sourceId":9124263,"sourceType":"datasetVersion"}],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
